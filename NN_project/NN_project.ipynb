{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ef9e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined records: 69\n"
     ]
    }
   ],
   "source": [
    "import ndjson\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Open and read the NDJSON file\n",
    "with open('D:\\\\Manager-angry-meter\\\\video-input\\\\videos.ndjson', 'r') as f:\n",
    "    data = ndjson.load(f)\n",
    "\n",
    "\n",
    "print(f\"Combined records: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "809fb392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cmhpke9p4cjbs0782sob057xx.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cmhpke9p4cjbt0782363n2x0u.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmhpke9p4cjbu0782assgc5x9.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmhpke9p4cjbv0782ah5bfrg4.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmhpke9p4cjbw0782wkp7yx3s.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id                   emotion\n",
       "0  cmhpke9p4cjbs0782sob057xx.mp4  [{'value': 'Surprised'}]\n",
       "1  cmhpke9p4cjbt0782363n2x0u.mp4  [{'value': 'Surprised'}]\n",
       "2  cmhpke9p4cjbu0782assgc5x9.mp4  [{'value': 'Surprised'}]\n",
       "3  cmhpke9p4cjbv0782ah5bfrg4.mp4  [{'value': 'Surprised'}]\n",
       "4  cmhpke9p4cjbw0782wkp7yx3s.mp4  [{'value': 'Surprised'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta = pd.DataFrame(data)\n",
    "df_meta.drop(columns=['media_attributes'], inplace=True)\n",
    "df_meta['data_row'] = df_meta['data_row'].astype(str).str[8:37]\n",
    "df_meta.rename(columns={'data_row': 'id'}, inplace=True)\n",
    "df_meta.rename(columns={'metadata_fields': 'emotion'}, inplace=True)\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbeafb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "\n",
      "Unique emotions found:\n",
      "['surprised' 'sad' 'neutral' 'happy' 'disgust' 'angry']\n"
     ]
    }
   ],
   "source": [
    "# --- ROBUST DATA CLEANING ---\n",
    "def clean_emotion_col(x):\n",
    "    # 1. If it's a list (e.g., ['angry']), grab the first item\n",
    "    if isinstance(x, list):\n",
    "        if len(x) > 0:\n",
    "            x = x[0]\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    # 2. If it's a dictionary (e.g., {'label': 'angry'}), grab the value\n",
    "    if isinstance(x, dict):\n",
    "        # Try common keys, or just grab the first value found\n",
    "        x = x.get('label', x.get('emotion', list(x.values())[0]))\n",
    "\n",
    "    # 3. Convert whatever is left to a string and lower-case it\n",
    "    text = str(x).lower()\n",
    "    \n",
    "    # 4. Remove any leftover garbage characters\n",
    "    text = text.replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "    return text\n",
    "\n",
    "# Apply the fix\n",
    "print(\"Cleaning data...\")\n",
    "df_meta['emotion'] = df_meta['emotion'].apply(clean_emotion_col)\n",
    "# --- VERIFY ---\n",
    "print(\"\\nUnique emotions found:\")\n",
    "print(df_meta['emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c7b40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Face Detector loaded successfully from: C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml\n",
      "ğŸšœ Starting HD Harvest from 69 videos...\n",
      "âœ… HD Harvest Complete!\n",
      "   Scanned 4677 total frames.\n",
      "   Saved 4669 high-quality face images to 'data/processed_frames_HD'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "video_folder = \"D:\\\\Manager-angry-meter\\\\data\\\\videos\"\n",
    "output_folder = \"data/processed_frames_HD\" \n",
    "img_size = (128, 128) \n",
    "\n",
    "# --- FIX IS HERE ---\n",
    "# 1. Define the path\n",
    "cascade_path = r\"C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# 2. Check if file exists (Crucial because Windows paths can be tricky)\n",
    "if not os.path.exists(cascade_path):\n",
    "    print(f\"âŒ ERROR: Cannot find the Cascade XML at: {cascade_path}\")\n",
    "    print(\"   Please check the path again.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Actually LOAD the classifier object\n",
    "face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "\n",
    "# Verify it loaded correctly\n",
    "if face_cascade.empty():\n",
    "    print(\"âŒ ERROR: Cascade file found but failed to load. Is it corrupt?\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"âœ… Face Detector loaded successfully from: {cascade_path}\")\n",
    "\n",
    "\n",
    "# --- START HARVESTING ---\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "print(f\"ğŸšœ Starting HD Harvest from {len(df_meta)} videos...\")\n",
    "\n",
    "count = 0\n",
    "faces_found_count = 0\n",
    "\n",
    "for index, row in df_meta.iterrows():\n",
    "    video_filename = row['id']\n",
    "    emotion_label = row['emotion']\n",
    "    \n",
    "    # Check extension\n",
    "    if not video_filename.endswith('.mp4'):\n",
    "        video_filename += \".mp4\"\n",
    "        \n",
    "    video_path = os.path.join(video_folder, video_filename)\n",
    "    \n",
    "    # Skip if video file missing\n",
    "    if not os.path.exists(video_path):\n",
    "        continue\n",
    "\n",
    "    # Create Emotion Folder\n",
    "    label_path = os.path.join(output_folder, emotion_label)\n",
    "    os.makedirs(label_path, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_id = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 1. Convert to Gray (Face detection needs grayscale)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 2. Detect Faces\n",
    "        # scaleFactor=1.1 -> Scans image at different sizes (10% increments)\n",
    "        # minNeighbors=4  -> Higher = fewer false positives, but might miss faces\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "        \n",
    "        # 3. If a face is found, crop it\n",
    "        if len(faces) > 0:\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Crop: [y_start:y_end, x_start:x_end]\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                try:\n",
    "                    # High Quality Resize\n",
    "                    face_resized = cv2.resize(face_img, img_size, interpolation=cv2.INTER_AREA)\n",
    "                    \n",
    "                    # Save\n",
    "                    clean_name = video_filename.replace('.mp4', '')\n",
    "                    save_name = f\"{clean_name}_f{frame_id}.jpg\"\n",
    "                    cv2.imwrite(os.path.join(label_path, save_name), face_resized)\n",
    "                    \n",
    "                    faces_found_count += 1\n",
    "                except Exception as e:\n",
    "                    pass # Skip if resize crashes (rare)\n",
    "                \n",
    "                # Only take the first face to avoid duplicates/crowd\n",
    "                break \n",
    "        \n",
    "        frame_id += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "print(f\"âœ… HD Harvest Complete!\")\n",
    "print(f\"   Scanned {count} total frames.\")\n",
    "print(f\"   Saved {faces_found_count} high-quality face images to '{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d2c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Training Data from data/processed_frames_HD...\n",
      "Found 3863 images belonging to 6 classes.\n",
      "ğŸ“‚ Loading Validation Data from data/processed_frames_HD...\n",
      "Found 964 images belonging to 6 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Training on HD Data...\n",
      "Epoch 1/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 265ms/step - accuracy: 0.3508 - loss: 1.5737 - val_accuracy: 0.3693 - val_loss: 1.4441\n",
      "Epoch 2/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 95ms/step - accuracy: 0.6032 - loss: 1.0307 - val_accuracy: 0.6214 - val_loss: 1.1729\n",
      "Epoch 3/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 103ms/step - accuracy: 0.7468 - loss: 0.6686 - val_accuracy: 0.7106 - val_loss: 1.1385\n",
      "Epoch 4/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 100ms/step - accuracy: 0.8123 - loss: 0.5074 - val_accuracy: 0.6276 - val_loss: 1.0377\n",
      "Epoch 5/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 99ms/step - accuracy: 0.8514 - loss: 0.4052 - val_accuracy: 0.5716 - val_loss: 1.2139\n",
      "Epoch 6/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 103ms/step - accuracy: 0.8796 - loss: 0.3317 - val_accuracy: 0.6649 - val_loss: 1.0316\n",
      "Epoch 7/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.8858 - loss: 0.3216 - val_accuracy: 0.6867 - val_loss: 1.0248\n",
      "Epoch 8/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.9021 - loss: 0.2579 - val_accuracy: 0.6494 - val_loss: 1.3534\n",
      "Epoch 9/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 102ms/step - accuracy: 0.9154 - loss: 0.2310 - val_accuracy: 0.5996 - val_loss: 1.4676\n",
      "Epoch 10/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 101ms/step - accuracy: 0.9060 - loss: 0.2357 - val_accuracy: 0.6992 - val_loss: 1.0358\n",
      "Epoch 11/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 98ms/step - accuracy: 0.9174 - loss: 0.2162 - val_accuracy: 0.7116 - val_loss: 1.3147\n",
      "Epoch 12/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9177 - loss: 0.1864 - val_accuracy: 0.6432 - val_loss: 1.3399\n",
      "Epoch 13/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9273 - loss: 0.1824 - val_accuracy: 0.6587 - val_loss: 1.4261\n",
      "Epoch 14/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9244 - loss: 0.1754 - val_accuracy: 0.6463 - val_loss: 1.3946\n",
      "Epoch 15/15\n",
      "\u001b[1m121/121\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9234 - loss: 0.1823 - val_accuracy: 0.6805 - val_loss: 1.2663\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "dataset_path = \"data/processed_frames_HD\" \n",
    "img_height, img_width = 128, 128 \n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‚ Loading Training Data from {dataset_path}...\")\n",
    "train_ds = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‚ Loading Validation Data from {dataset_path}...\")\n",
    "val_ds = datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(train_ds.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"ğŸš€ Starting Training on HD Data...\")\n",
    "history = model.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3162bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined records: 50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ndjson\n",
    "\n",
    "# Open and read the NDJSON file\n",
    "with open('Audio.ndjson', 'r') as f:\n",
    "    data_audio = ndjson.load(f)\n",
    "\n",
    "\n",
    "print(f\"Combined records: {len(data_audio)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa3eb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cmi6kb42m14t80768b3k66mu7.WAV</td>\n",
       "      <td>[{'value': 'Angry'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cmi6kb42m14t907681xw39xws.WAV</td>\n",
       "      <td>[{'value': 'Angry'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmi6kb42m14ta0768107er0vz.WAV</td>\n",
       "      <td>[{'value': 'Angry'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmi6kb42m14tb0768roy88bhh.WAV</td>\n",
       "      <td>[{'value': 'Angry'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmi6kb42m14tc076895ghj9du.WAV</td>\n",
       "      <td>[{'value': 'Angry'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id               emotion\n",
       "0  cmi6kb42m14t80768b3k66mu7.WAV  [{'value': 'Angry'}]\n",
       "1  cmi6kb42m14t907681xw39xws.WAV  [{'value': 'Angry'}]\n",
       "2  cmi6kb42m14ta0768107er0vz.WAV  [{'value': 'Angry'}]\n",
       "3  cmi6kb42m14tb0768roy88bhh.WAV  [{'value': 'Angry'}]\n",
       "4  cmi6kb42m14tc076895ghj9du.WAV  [{'value': 'Angry'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta_audio = pd.DataFrame(data_audio)\n",
    "df_meta_audio.drop(columns=['media_attributes'], inplace=True)\n",
    "df_meta_audio['data_row'] = df_meta_audio['data_row'].astype(str).str[8:37]\n",
    "df_meta_audio.rename(columns={'data_row': 'id'}, inplace=True)\n",
    "df_meta_audio.rename(columns={'metadata_fields': 'emotion'}, inplace=True)\n",
    "df_meta_audio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1b6959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 04:43:15.829161: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-22 04:43:15.882313: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-22 04:43:16.881458: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/adham/gpu_env/lib/python3.12/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU Detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow is using the GPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ… GPU Detected: {gpus}\")\n",
    "    print(\"TensorFlow is using the GPU.\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c161863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading Pretrained Wav2Vec2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1766371400.667465   19784 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "Some layers from the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing TFWav2Vec2Model: ['lm_head', 'dropout_50']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFWav2Vec2Model were initialized from the model checkpoint at facebook/wav2vec2-base-960h.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Extracting Deep Features from 50 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/50 [00:00<?, ?it/s]TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "2025-12-22 04:43:22.892995: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91700\n",
      "2025-12-22 04:43:24.051349: I external/local_xla/xla/service/service.cc:163] XLA service 0x41107830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-12-22 04:43:24.051380: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "I0000 00:00:1766371404.270646   19784 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:13<00:00,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extraction Complete! Feature Shape: (50, 768)\n",
      "ğŸ’¾ Saving 50 samples to disk...\n",
      "âœ… Successfully saved 'X_deep_features.pkl' and 'y_deep_labels.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, TFWav2Vec2Model\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "video_folder = \"data\"\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\" # The Industry Standard for Speech\n",
    "MAX_DURATION = 3 # seconds\n",
    "\n",
    "# 1. LOAD PRETRAINED MODEL (The \"Brain\")\n",
    "print(\"ğŸ“¥ Loading Pretrained Wav2Vec2 Model...\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "# We load the TensorFlow version of the model\n",
    "# output_hidden_states=True lets us peek inside the \"brain\"\n",
    "# Force it to use standard weights to avoid the iteration error\n",
    "base_model = TFWav2Vec2Model.from_pretrained(MODEL_NAME, use_safetensors=False)\n",
    "def extract_deep_features(path):\n",
    "    try:\n",
    "        # A. Load Audio (16kHz is mandatory for Wav2Vec2)\n",
    "        audio, sr = librosa.load(path, sr=16000, duration=MAX_DURATION)\n",
    "        \n",
    "        # B. Pad/Truncate to ensure consistent length\n",
    "        # 3 seconds * 16000 Hz = 48000 samples\n",
    "        target_len = 16000 * MAX_DURATION\n",
    "        if len(audio) < target_len:\n",
    "            audio = np.pad(audio, (0, target_len - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:target_len]\n",
    "\n",
    "        # C. Prepare Input for the Model\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"tf\")\n",
    "        \n",
    "        # D. Run through Pretrained Model\n",
    "        outputs = base_model(inputs.input_values)\n",
    "        \n",
    "        # E. Get the \"Embeddings\" (The last hidden state)\n",
    "        # Shape: (1, Sequence_Length, 768) -> We average it to get (768,)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        features = tf.reduce_mean(hidden_states, axis=1) \n",
    "        \n",
    "        return features.numpy().flatten()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- HARVEST LOOP ---\n",
    "deep_features = []\n",
    "labels = []\n",
    "\n",
    "print(f\"ğŸ§  Extracting Deep Features from {len(df_meta_audio)} videos...\")\n",
    "\n",
    "for index, row in tqdm(df_meta_audio.iterrows(), total=len(df_meta_audio)):\n",
    "    filename = row['id']\n",
    "    path = os.path.join(video_folder, filename)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        feats = extract_deep_features(path)\n",
    "        if feats is not None:\n",
    "            deep_features.append(feats)\n",
    "            labels.append(row['emotion'])\n",
    "\n",
    "# Save to disk so you don't have to run this again\n",
    "X_deep = np.array(deep_features)\n",
    "print(f\"âœ… Extraction Complete! Feature Shape: {X_deep.shape}\") \n",
    "# Shape should be (Num_Videos, 768) -> 768 is the size of Wav2Vec2 embeddings\n",
    "\n",
    "if len(deep_features) > 0:\n",
    "    print(f\"ğŸ’¾ Saving {len(deep_features)} samples to disk...\")\n",
    "    \n",
    "    # Save the features (X)\n",
    "    joblib.dump(X_deep, 'X_deep_features.pkl')\n",
    "    \n",
    "    # Save the labels (y)\n",
    "    joblib.dump(labels, 'y_deep_labels.pkl')\n",
    "    \n",
    "    print(\"âœ… Successfully saved 'X_deep_features.pkl' and 'y_deep_labels.pkl'\")\n",
    "else:\n",
    "    print(\"âš ï¸ No features were extracted. Nothing to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c522c5b-d51a-435a-8b39-dc850c632ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "\n",
      "Unique emotions found:\n",
      "['angry' 'disgust' 'happy' 'neutral' 'sad' 'surprised']\n"
     ]
    }
   ],
   "source": [
    "# --- ROBUST DATA CLEANING ---\n",
    "def clean_emotion_col(x):\n",
    "    # 1. If it's a list (e.g., ['angry']), grab the first item\n",
    "    if isinstance(x, list):\n",
    "        if len(x) > 0:\n",
    "            x = x[0]\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    # 2. If it's a dictionary (e.g., {'label': 'angry'}), grab the value\n",
    "    if isinstance(x, dict):\n",
    "        # Try common keys, or just grab the first value found\n",
    "        x = x.get('label', x.get('emotion', list(x.values())[0]))\n",
    "\n",
    "    # 3. Convert whatever is left to a string and lower-case it\n",
    "    text = str(x).lower()\n",
    "    \n",
    "    # 4. Remove any leftover garbage characters\n",
    "    text = text.replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "    return text\n",
    "X_deep = np.array(deep_features)\n",
    "y_deep = pd.DataFrame(labels, columns=['emotion'])\n",
    "# Apply the fix\n",
    "print(\"Cleaning data...\")\n",
    "y_deep['emotion'] = y_deep['emotion'].apply(clean_emotion_col)\n",
    "\n",
    "print(\"\\nUnique emotions found:\")\n",
    "print(y_deep['emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d0dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Labels encoded. Classes found: ['angry' 'disgust' 'happy' 'neutral' 'sad' 'surprised']\n",
      "ğŸš€ Training Transfer Learning Model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 04:43:36.799986: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-12-22 04:43:37.054878: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-22 04:43:37.054989: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-22 04:43:37.055008: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-22 04:43:37.055017: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-12-22 04:43:37.560445: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1024', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-12-22 04:43:39.302600: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1504', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-12-22 04:43:39.359046: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1487', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3s/step - accuracy: 0.2750 - loss: 3.0193 - val_accuracy: 0.3000 - val_loss: 1.7976\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.3500 - loss: 2.1280 - val_accuracy: 0.4000 - val_loss: 1.7642\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.4250 - loss: 1.5324 - val_accuracy: 0.3000 - val_loss: 1.7423\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.3500 - loss: 1.7275 - val_accuracy: 0.2000 - val_loss: 1.7227\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5500 - loss: 1.3293 - val_accuracy: 0.3000 - val_loss: 1.7063\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5000 - loss: 1.1460 - val_accuracy: 0.4000 - val_loss: 1.6938\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 1.5558 - val_accuracy: 0.4000 - val_loss: 1.6872\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 1.0446 - val_accuracy: 0.4000 - val_loss: 1.6847\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5500 - loss: 1.1069 - val_accuracy: 0.4000 - val_loss: 1.6776\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 0.9573 - val_accuracy: 0.4000 - val_loss: 1.6725\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6250 - loss: 0.9115 - val_accuracy: 0.4000 - val_loss: 1.6649\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6250 - loss: 0.8510 - val_accuracy: 0.4000 - val_loss: 1.6593\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6500 - loss: 0.9148 - val_accuracy: 0.4000 - val_loss: 1.6523\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6250 - loss: 0.9709 - val_accuracy: 0.4000 - val_loss: 1.6489\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6750 - loss: 0.8741 - val_accuracy: 0.4000 - val_loss: 1.6479\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7250 - loss: 0.8303 - val_accuracy: 0.4000 - val_loss: 1.6487\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7750 - loss: 0.5583 - val_accuracy: 0.3000 - val_loss: 1.6490\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7250 - loss: 0.8243 - val_accuracy: 0.3000 - val_loss: 1.6502\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8000 - loss: 0.7929 - val_accuracy: 0.3000 - val_loss: 1.6515\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7750 - loss: 0.6490 - val_accuracy: 0.4000 - val_loss: 1.6564\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8250 - loss: 0.5091 - val_accuracy: 0.4000 - val_loss: 1.6618\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8250 - loss: 0.6277 - val_accuracy: 0.3000 - val_loss: 1.6709\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8000 - loss: 0.7182 - val_accuracy: 0.3000 - val_loss: 1.6758\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6500 - loss: 0.7741 - val_accuracy: 0.3000 - val_loss: 1.6793\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7750 - loss: 0.7127 - val_accuracy: 0.3000 - val_loss: 1.6766\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7500 - loss: 0.7122 - val_accuracy: 0.3000 - val_loss: 1.6707\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8000 - loss: 0.6198 - val_accuracy: 0.3000 - val_loss: 1.6647\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7500 - loss: 0.6223 - val_accuracy: 0.3000 - val_loss: 1.6579\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7750 - loss: 0.6578 - val_accuracy: 0.3000 - val_loss: 1.6509\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8500 - loss: 0.4477 - val_accuracy: 0.3000 - val_loss: 1.6494\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8750 - loss: 0.4221 - val_accuracy: 0.4000 - val_loss: 1.6479\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8750 - loss: 0.3751 - val_accuracy: 0.4000 - val_loss: 1.6394\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8250 - loss: 0.4035 - val_accuracy: 0.4000 - val_loss: 1.6335\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8000 - loss: 0.5448 - val_accuracy: 0.4000 - val_loss: 1.6287\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8750 - loss: 0.4016 - val_accuracy: 0.4000 - val_loss: 1.6303\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8500 - loss: 0.3241 - val_accuracy: 0.4000 - val_loss: 1.6360\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8750 - loss: 0.4830 - val_accuracy: 0.4000 - val_loss: 1.6435\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8250 - loss: 0.4768 - val_accuracy: 0.4000 - val_loss: 1.6462\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9000 - loss: 0.3906 - val_accuracy: 0.4000 - val_loss: 1.6404\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9250 - loss: 0.3951 - val_accuracy: 0.4000 - val_loss: 1.6390\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9500 - loss: 0.2367 - val_accuracy: 0.4000 - val_loss: 1.6446\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9250 - loss: 0.2177 - val_accuracy: 0.3000 - val_loss: 1.6506\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9000 - loss: 0.3421 - val_accuracy: 0.3000 - val_loss: 1.6548\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8750 - loss: 0.3972 - val_accuracy: 0.4000 - val_loss: 1.6524\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9000 - loss: 0.2911 - val_accuracy: 0.4000 - val_loss: 1.6536\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8750 - loss: 0.4495 - val_accuracy: 0.4000 - val_loss: 1.6576\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9250 - loss: 0.2952 - val_accuracy: 0.4000 - val_loss: 1.6623\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.2243 - val_accuracy: 0.4000 - val_loss: 1.6665\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9500 - loss: 0.3001 - val_accuracy: 0.4000 - val_loss: 1.6677\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9000 - loss: 0.2960 - val_accuracy: 0.4000 - val_loss: 1.6700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Prepare Labels\n",
    "# ---------------------------------------------------------\n",
    "# ERROR WAS HERE: You passed 'labels' (dirty) instead of y_deep['emotion'] (clean)\n",
    "lb = LabelEncoder()\n",
    "y_final = to_categorical(lb.fit_transform(y_deep['emotion']))\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(f\"âœ… Labels encoded. Classes found: {lb.classes_}\")\n",
    "\n",
    "# 2. Split\n",
    "# Use y_final here, not y_deep (to avoid variable confusion)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_deep, y_final, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. BUILD YOUR CUSTOM MODEL\n",
    "model_transfer = Sequential([\n",
    "    # Input is 768 (The output of Wav2Vec2)\n",
    "    tf.keras.Input(shape=(768,)),\n",
    "    \n",
    "    # Added Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # Added Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(y_final.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 4. Train\n",
    "model_transfer.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"ğŸš€ Training Transfer Learning Model...\")\n",
    "history = model_transfer.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "\n",
    "# 5. Save\n",
    "model_transfer.save('wav2vec2_transfer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b926d1e-5592-4c6d-a2b8-2ef2e8815945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading Pretrained Wav2Vec2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "Some layers from the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing TFWav2Vec2Model: ['lm_head', 'dropout_50']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFWav2Vec2Model were initialized from the model checkpoint at facebook/wav2vec2-base-960h.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Scanning recursive directories in: kaggle-data/audio\n",
      "ğŸ§  Extracting features...\n",
      "\n",
      "âœ… Extraction Complete!\n",
      "   - Samples: 50\n",
      "   - Feature Shape: (50, 768)\n",
      "ğŸ’¾ Saving to disk...\n",
      "âœ… Saved 'X_deep_features_kaggle.pkl' and 'y_deep_labels_kaggle.pkl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, TFWav2Vec2Model\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "ROOT_DIR = r\"kaggle-data/audio\"  # Your data folder\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\"\n",
    "MAX_DURATION = 3 # seconds\n",
    "\n",
    "# --- 0. DEFINE EMOTION MAP (RAVDESS Standard) ---\n",
    "# Based on the filename format \"03-01-05...\"\n",
    "EMOTION_MAP = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'neutral',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# --- 1. LOAD PRETRAINED MODEL ---\n",
    "print(\"ğŸ“¥ Loading Pretrained Wav2Vec2 Model...\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "base_model = TFWav2Vec2Model.from_pretrained(MODEL_NAME, use_safetensors=False)\n",
    "\n",
    "def extract_deep_features(path):\n",
    "    try:\n",
    "        # A. Load Audio (16kHz is mandatory for Wav2Vec2)\n",
    "        audio, sr = librosa.load(path, sr=16000, duration=MAX_DURATION)\n",
    "        \n",
    "        # B. Pad/Truncate\n",
    "        target_len = 16000 * MAX_DURATION\n",
    "        if len(audio) < target_len:\n",
    "            audio = np.pad(audio, (0, target_len - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:target_len]\n",
    "\n",
    "        # C. Process\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"tf\")\n",
    "        outputs = base_model(inputs.input_values)\n",
    "        \n",
    "        # D. Average Embeddings\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        features = tf.reduce_mean(hidden_states, axis=1) \n",
    "        \n",
    "        return features.numpy().flatten()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 2. HARVEST LOOP (Refactored) ---\n",
    "deep_features = []\n",
    "labels = []\n",
    "filenames_log = [] # Optional: keep track of which file produced which feature\n",
    "\n",
    "print(f\"ğŸ“‚ Scanning recursive directories in: {ROOT_DIR}\")\n",
    "print(\"ğŸ§  Extracting features...\")\n",
    "\n",
    "# We use os.walk to find all files recursively\n",
    "for root, dirs, files in os.walk(ROOT_DIR):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith('.wav'):\n",
    "            \n",
    "            # --- A. PARSE EMOTION ---\n",
    "            # Format: 03-01-05-01-01-01-01.wav\n",
    "            try:\n",
    "                parts = filename.split('-')\n",
    "                if len(parts) >= 3:\n",
    "                    code = parts[2] # The 3rd part is the emotion\n",
    "                    emotion_label = EMOTION_MAP.get(code, 'unknown')\n",
    "                else:\n",
    "                    emotion_label = 'unknown'\n",
    "            except:\n",
    "                emotion_label = 'unknown'\n",
    "\n",
    "            # Skip if we couldn't identify the emotion\n",
    "            if emotion_label == 'unknown': \n",
    "                continue\n",
    "\n",
    "            # --- B. EXTRACT FEATURES ---\n",
    "            file_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Print progress every now and then (optional)\n",
    "            # print(f\"Processing: {filename} -> {emotion_label}\")\n",
    "\n",
    "            feats = extract_deep_features(file_path)\n",
    "            \n",
    "            if feats is not None:\n",
    "                deep_features.append(feats)\n",
    "                labels.append(emotion_label)\n",
    "                filenames_log.append(filename)\n",
    "\n",
    "# --- 3. SAVE ---\n",
    "if len(deep_features) > 0:\n",
    "    X_deep_kaggle = np.array(deep_features)\n",
    "    print(f\"\\nâœ… Extraction Complete!\")\n",
    "    print(f\"   - Samples: {len(X_deep)}\")\n",
    "    print(f\"   - Feature Shape: {X_deep.shape}\")\n",
    "    \n",
    "    print(\"ğŸ’¾ Saving to disk...\")\n",
    "    joblib.dump(X_deep_kaggle, 'X_deep_features_kaggle.pkl')\n",
    "    joblib.dump(labels, 'y_deep_labels_kaggle.pkl')\n",
    "    print(\"âœ… Saved 'X_deep_features_kaggle.pkl' and 'y_deep_labels_kaggle.pkl'\")\n",
    "else:\n",
    "    print(\"âš ï¸ No features were extracted. Check your folder path or filename format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "446efade-95c8-4ffc-a06d-a986538c7a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Labels encoded. Classes found: ['angry' 'disgust' 'fearful' 'happy' 'neutral' 'sad' 'surprised']\n",
      "ğŸ“Š Training Data Shape: (384, 768)\n",
      "ğŸ“Š Testing Data Shape: (96, 768)\n",
      "ğŸš€ Training Transfer Learning Model...\n",
      "Epoch 1/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.1328 - loss: 2.8311 - val_accuracy: 0.1354 - val_loss: 1.9512\n",
      "Epoch 2/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1797 - loss: 2.4157 - val_accuracy: 0.2188 - val_loss: 1.9640\n",
      "Epoch 3/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2005 - loss: 2.3699 - val_accuracy: 0.1979 - val_loss: 1.9501\n",
      "Epoch 4/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.1901 - loss: 2.2935 - val_accuracy: 0.2083 - val_loss: 1.9288\n",
      "Epoch 5/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1849 - loss: 2.1791 - val_accuracy: 0.1979 - val_loss: 1.9301\n",
      "Epoch 6/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2396 - loss: 2.0714 - val_accuracy: 0.1562 - val_loss: 1.9255\n",
      "Epoch 7/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2344 - loss: 2.1726 - val_accuracy: 0.1354 - val_loss: 1.9360\n",
      "Epoch 8/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1979 - loss: 2.1588 - val_accuracy: 0.1667 - val_loss: 1.9355\n",
      "Epoch 9/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2318 - loss: 2.0918 - val_accuracy: 0.1146 - val_loss: 1.9550\n",
      "Epoch 10/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2604 - loss: 1.9990 - val_accuracy: 0.1250 - val_loss: 1.9586\n",
      "Epoch 11/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2370 - loss: 1.9791 - val_accuracy: 0.1250 - val_loss: 1.9695\n",
      "Epoch 12/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2865 - loss: 1.9831 - val_accuracy: 0.1458 - val_loss: 1.9744\n",
      "Epoch 13/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2786 - loss: 1.9519 - val_accuracy: 0.1562 - val_loss: 1.9572\n",
      "Epoch 14/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2552 - loss: 1.9515 - val_accuracy: 0.1562 - val_loss: 1.9505\n",
      "Epoch 15/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2786 - loss: 1.9232 - val_accuracy: 0.1771 - val_loss: 1.9629\n",
      "Epoch 16/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2708 - loss: 1.8988 - val_accuracy: 0.1771 - val_loss: 1.9384\n",
      "Epoch 17/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2682 - loss: 1.8920 - val_accuracy: 0.1562 - val_loss: 1.9534\n",
      "Epoch 18/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.2604 - loss: 1.8763 - val_accuracy: 0.1979 - val_loss: 1.8900\n",
      "Epoch 19/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3073 - loss: 1.8567 - val_accuracy: 0.2188 - val_loss: 1.8401\n",
      "Epoch 20/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3385 - loss: 1.8234 - val_accuracy: 0.1875 - val_loss: 1.8517\n",
      "Epoch 21/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2839 - loss: 1.8628 - val_accuracy: 0.1771 - val_loss: 1.8526\n",
      "Epoch 22/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3724 - loss: 1.7209 - val_accuracy: 0.1875 - val_loss: 1.8763\n",
      "Epoch 23/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3099 - loss: 1.6986 - val_accuracy: 0.2188 - val_loss: 1.8561\n",
      "Epoch 24/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3411 - loss: 1.7366 - val_accuracy: 0.1875 - val_loss: 1.8685\n",
      "Epoch 25/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3307 - loss: 1.7635 - val_accuracy: 0.1562 - val_loss: 2.0034\n",
      "Epoch 26/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3359 - loss: 1.6930 - val_accuracy: 0.1667 - val_loss: 1.9761\n",
      "Epoch 27/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3464 - loss: 1.7297 - val_accuracy: 0.1562 - val_loss: 1.9680\n",
      "Epoch 28/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3490 - loss: 1.7461 - val_accuracy: 0.1875 - val_loss: 1.9733\n",
      "Epoch 29/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3776 - loss: 1.6384 - val_accuracy: 0.1562 - val_loss: 2.1102\n",
      "Epoch 30/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3281 - loss: 1.6775 - val_accuracy: 0.2292 - val_loss: 1.8961\n",
      "Epoch 31/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3880 - loss: 1.5873 - val_accuracy: 0.1875 - val_loss: 1.8994\n",
      "Epoch 32/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3906 - loss: 1.5589 - val_accuracy: 0.2292 - val_loss: 1.8517\n",
      "Epoch 33/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3880 - loss: 1.5565 - val_accuracy: 0.2292 - val_loss: 1.8312\n",
      "Epoch 34/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3568 - loss: 1.5853 - val_accuracy: 0.2604 - val_loss: 1.7877\n",
      "Epoch 35/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3958 - loss: 1.5402 - val_accuracy: 0.1562 - val_loss: 1.8906\n",
      "Epoch 36/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3984 - loss: 1.5641 - val_accuracy: 0.1875 - val_loss: 1.8856\n",
      "Epoch 37/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4036 - loss: 1.6032 - val_accuracy: 0.1562 - val_loss: 1.9991\n",
      "Epoch 38/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3854 - loss: 1.5598 - val_accuracy: 0.2188 - val_loss: 1.8492\n",
      "Epoch 39/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3984 - loss: 1.5367 - val_accuracy: 0.2812 - val_loss: 1.7988\n",
      "Epoch 40/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4089 - loss: 1.5177 - val_accuracy: 0.2708 - val_loss: 1.7369\n",
      "Epoch 41/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4115 - loss: 1.5282 - val_accuracy: 0.3125 - val_loss: 1.7547\n",
      "Epoch 42/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3932 - loss: 1.5133 - val_accuracy: 0.2812 - val_loss: 1.7369\n",
      "Epoch 43/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3620 - loss: 1.6048 - val_accuracy: 0.3125 - val_loss: 1.6972\n",
      "Epoch 44/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4219 - loss: 1.4696 - val_accuracy: 0.4271 - val_loss: 1.7176\n",
      "Epoch 45/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4323 - loss: 1.4693 - val_accuracy: 0.2708 - val_loss: 1.7257\n",
      "Epoch 46/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4062 - loss: 1.4929 - val_accuracy: 0.3333 - val_loss: 1.7375\n",
      "Epoch 47/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3906 - loss: 1.5440 - val_accuracy: 0.1875 - val_loss: 2.0773\n",
      "Epoch 48/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4453 - loss: 1.4566 - val_accuracy: 0.2708 - val_loss: 1.8662\n",
      "Epoch 49/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4349 - loss: 1.4581 - val_accuracy: 0.4062 - val_loss: 1.7527\n",
      "Epoch 50/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4010 - loss: 1.4966 - val_accuracy: 0.2812 - val_loss: 2.1189\n",
      "âœ… Model saved as 'wav2vec2_transfer_model_kaggle.keras'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- 1. PREPARE LABELS ---\n",
    "# We use 'labels_kaggle' (the list) directly. \n",
    "# No need for a DataFrame ('y_deep') here unless you want to save it later.\n",
    "lb = LabelEncoder()\n",
    "y_final = to_categorical(lb.fit_transform(labels))\n",
    "\n",
    "print(f\"âœ… Labels encoded. Classes found: {lb.classes_}\")\n",
    "\n",
    "# --- 2. SPLIT ---\n",
    "# Updated to use 'X_deep_kaggle'\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_deep_kaggle, \n",
    "    y_final, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š Training Data Shape: {X_train.shape}\")\n",
    "print(f\"ğŸ“Š Testing Data Shape: {X_test.shape}\")\n",
    "\n",
    "# --- 3. BUILD YOUR CUSTOM MODEL ---\n",
    "model_transfer = Sequential([\n",
    "    # Input is 768 (The output of Wav2Vec2)\n",
    "    # Using dynamic shape is safer: (X_deep_kaggle.shape[1],)\n",
    "    tf.keras.Input(shape=(768,)),\n",
    "    \n",
    "    # Added Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    # Added Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(y_final.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# --- 4. TRAIN ---\n",
    "model_transfer.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"ğŸš€ Training Transfer Learning Model...\")\n",
    "history = model_transfer.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "\n",
    "# --- 5. SAVE ---\n",
    "# Updated filename to reflect the Kaggle dataset\n",
    "model_transfer.save('wav2vec2_transfer_model_kaggle.keras')\n",
    "print(\"âœ… Model saved as 'wav2vec2_transfer_model_kaggle.keras'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
