{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ef9e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined records: 69\n"
     ]
    }
   ],
   "source": [
    "import ndjson\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Open and read the NDJSON file\n",
    "with open('D:\\\\MANAGER-ANGRY-METER\\\\video-input\\\\videos.ndjson', 'r') as f:\n",
    "    data = ndjson.load(f)\n",
    "\n",
    "\n",
    "print(f\"Combined records: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809fb392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cmhpke9p4cjbs0782sob057xx.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cmhpke9p4cjbt0782363n2x0u.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmhpke9p4cjbu0782assgc5x9.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmhpke9p4cjbv0782ah5bfrg4.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmhpke9p4cjbw0782wkp7yx3s.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id                   emotion\n",
       "0  cmhpke9p4cjbs0782sob057xx.mp4  [{'value': 'Surprised'}]\n",
       "1  cmhpke9p4cjbt0782363n2x0u.mp4  [{'value': 'Surprised'}]\n",
       "2  cmhpke9p4cjbu0782assgc5x9.mp4  [{'value': 'Surprised'}]\n",
       "3  cmhpke9p4cjbv0782ah5bfrg4.mp4  [{'value': 'Surprised'}]\n",
       "4  cmhpke9p4cjbw0782wkp7yx3s.mp4  [{'value': 'Surprised'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta = pd.DataFrame(data)\n",
    "df_meta.drop(columns=['media_attributes'], inplace=True)\n",
    "df_meta['data_row'] = df_meta['data_row'].astype(str).str[8:37]\n",
    "df_meta.rename(columns={'data_row': 'id'}, inplace=True)\n",
    "df_meta.rename(columns={'metadata_fields': 'emotion'}, inplace=True)\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbeafb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n",
      "\n",
      "Unique emotions found:\n",
      "['surprised' 'sad' 'neutral' 'happy' 'disgust' 'angry']\n"
     ]
    }
   ],
   "source": [
    "# --- ROBUST DATA CLEANING ---\n",
    "def clean_emotion_col(x):\n",
    "    # 1. If it's a list (e.g., ['angry']), grab the first item\n",
    "    if isinstance(x, list):\n",
    "        if len(x) > 0:\n",
    "            x = x[0]\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "\n",
    "    # 2. If it's a dictionary (e.g., {'label': 'angry'}), grab the value\n",
    "    if isinstance(x, dict):\n",
    "        # Try common keys, or just grab the first value found\n",
    "        x = x.get('label', x.get('emotion', list(x.values())[0]))\n",
    "\n",
    "    # 3. Convert whatever is left to a string and lower-case it\n",
    "    text = str(x).lower()\n",
    "    \n",
    "    # 4. Remove any leftover garbage characters\n",
    "    text = text.replace(\"'\", \"\").replace('\"', \"\").strip()\n",
    "    return text\n",
    "\n",
    "# Apply the fix\n",
    "print(\"Cleaning data...\")\n",
    "df_meta['emotion'] = df_meta['emotion'].apply(clean_emotion_col)\n",
    "# --- VERIFY ---\n",
    "print(\"\\nUnique emotions found:\")\n",
    "print(df_meta['emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c7b40d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Face Detector loaded successfully from: C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml\n",
      "ğŸšœ Starting HD Harvest from 69 videos...\n",
      "âœ… HD Harvest Complete!\n",
      "   Scanned 4677 total frames.\n",
      "   Saved 4669 high-quality face images to 'data/processed_frames_HD'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "video_folder = \"D:\\\\machine project\\\\data\\\\videos\"\n",
    "output_folder = \"data/processed_frames_HD\" \n",
    "img_size = (128, 128) \n",
    "\n",
    "# --- FIX IS HERE ---\n",
    "# 1. Define the path\n",
    "cascade_path = r\"C:\\Users\\PC\\anaconda3\\Lib\\site-packages\\cv2\\data\\haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# 2. Check if file exists (Crucial because Windows paths can be tricky)\n",
    "if not os.path.exists(cascade_path):\n",
    "    print(f\"âŒ ERROR: Cannot find the Cascade XML at: {cascade_path}\")\n",
    "    print(\"   Please check the path again.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Actually LOAD the classifier object\n",
    "face_cascade = cv2.CascadeClassifier(cascade_path)\n",
    "\n",
    "# Verify it loaded correctly\n",
    "if face_cascade.empty():\n",
    "    print(\"âŒ ERROR: Cascade file found but failed to load. Is it corrupt?\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"âœ… Face Detector loaded successfully from: {cascade_path}\")\n",
    "\n",
    "\n",
    "# --- START HARVESTING ---\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "print(f\"ğŸšœ Starting HD Harvest from {len(df_meta)} videos...\")\n",
    "\n",
    "count = 0\n",
    "faces_found_count = 0\n",
    "\n",
    "for index, row in df_meta.iterrows():\n",
    "    video_filename = row['id']\n",
    "    emotion_label = row['emotion']\n",
    "    \n",
    "    # Check extension\n",
    "    if not video_filename.endswith('.mp4'):\n",
    "        video_filename += \".mp4\"\n",
    "        \n",
    "    video_path = os.path.join(video_folder, video_filename)\n",
    "    \n",
    "    # Skip if video file missing\n",
    "    if not os.path.exists(video_path):\n",
    "        continue\n",
    "\n",
    "    # Create Emotion Folder\n",
    "    label_path = os.path.join(output_folder, emotion_label)\n",
    "    os.makedirs(label_path, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_id = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # 1. Convert to Gray (Face detection needs grayscale)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 2. Detect Faces\n",
    "        # scaleFactor=1.1 -> Scans image at different sizes (10% increments)\n",
    "        # minNeighbors=4  -> Higher = fewer false positives, but might miss faces\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\n",
    "        \n",
    "        # 3. If a face is found, crop it\n",
    "        if len(faces) > 0:\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Crop: [y_start:y_end, x_start:x_end]\n",
    "                face_img = frame[y:y+h, x:x+w]\n",
    "                \n",
    "                try:\n",
    "                    # High Quality Resize\n",
    "                    face_resized = cv2.resize(face_img, img_size, interpolation=cv2.INTER_AREA)\n",
    "                    \n",
    "                    # Save\n",
    "                    clean_name = video_filename.replace('.mp4', '')\n",
    "                    save_name = f\"{clean_name}_f{frame_id}.jpg\"\n",
    "                    cv2.imwrite(os.path.join(label_path, save_name), face_resized)\n",
    "                    \n",
    "                    faces_found_count += 1\n",
    "                except Exception as e:\n",
    "                    pass # Skip if resize crashes (rare)\n",
    "                \n",
    "                # Only take the first face to avoid duplicates/crowd\n",
    "                break \n",
    "        \n",
    "        frame_id += 1\n",
    "        count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "print(f\"âœ… HD Harvest Complete!\")\n",
    "print(f\"   Scanned {count} total frames.\")\n",
    "print(f\"   Saved {faces_found_count} high-quality face images to '{output_folder}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147d2c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading Training Data from data/processed_frames_HD...\n",
      "Found 3622 images belonging to 6 classes.\n",
      "ğŸ“‚ Loading Validation Data from data/processed_frames_HD...\n",
      "Found 903 images belonging to 6 classes.\n",
      "ğŸš€ Starting Training on HD Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 227ms/step - accuracy: 0.4412 - loss: 1.4205 - val_accuracy: 0.5194 - val_loss: 1.3153\n",
      "Epoch 2/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 98ms/step - accuracy: 0.6676 - loss: 0.9010 - val_accuracy: 0.6002 - val_loss: 1.0317\n",
      "Epoch 3/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 97ms/step - accuracy: 0.7852 - loss: 0.5919 - val_accuracy: 0.6600 - val_loss: 1.0751\n",
      "Epoch 4/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 94ms/step - accuracy: 0.8490 - loss: 0.4146 - val_accuracy: 0.5703 - val_loss: 1.4805\n",
      "Epoch 5/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 95ms/step - accuracy: 0.8876 - loss: 0.3203 - val_accuracy: 0.6412 - val_loss: 1.3061\n",
      "Epoch 6/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 95ms/step - accuracy: 0.9194 - loss: 0.2351 - val_accuracy: 0.5327 - val_loss: 2.0926\n",
      "Epoch 7/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 97ms/step - accuracy: 0.9224 - loss: 0.2223 - val_accuracy: 0.6711 - val_loss: 1.6405\n",
      "Epoch 8/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 95ms/step - accuracy: 0.9268 - loss: 0.2050 - val_accuracy: 0.7065 - val_loss: 1.3381\n",
      "Epoch 9/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 94ms/step - accuracy: 0.9495 - loss: 0.1403 - val_accuracy: 0.7287 - val_loss: 1.2579\n",
      "Epoch 10/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 95ms/step - accuracy: 0.9464 - loss: 0.1516 - val_accuracy: 0.6678 - val_loss: 1.6322\n",
      "Epoch 11/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 96ms/step - accuracy: 0.9569 - loss: 0.1253 - val_accuracy: 0.6932 - val_loss: 1.7522\n",
      "Epoch 12/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 96ms/step - accuracy: 0.9613 - loss: 0.1110 - val_accuracy: 0.7564 - val_loss: 1.4568\n",
      "Epoch 13/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 100ms/step - accuracy: 0.9691 - loss: 0.1035 - val_accuracy: 0.6523 - val_loss: 1.4282\n",
      "Epoch 14/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 97ms/step - accuracy: 0.9671 - loss: 0.0949 - val_accuracy: 0.6944 - val_loss: 1.6122\n",
      "Epoch 15/15\n",
      "\u001b[1m114/114\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 98ms/step - accuracy: 0.9749 - loss: 0.0822 - val_accuracy: 0.7530 - val_loss: 2.0347\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We use the HD folder for BOTH training and validation\n",
    "dataset_path = \"data/processed_frames_HD\" \n",
    "img_height, img_width = 128, 128  # Match the size you harvested!\n",
    "\n",
    "# 1. LOAD DATA\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,      # Normalize\n",
    "    validation_split=0.2  # 20% split\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‚ Loading Training Data from {dataset_path}...\")\n",
    "train_ds = datagen.flow_from_directory(\n",
    "    dataset_path,             # FIXED PATH\n",
    "    target_size=(img_height, img_width), # FIXED SIZE (128x128)\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“‚ Loading Validation Data from {dataset_path}...\")\n",
    "val_ds = datagen.flow_from_directory(\n",
    "    dataset_path,             # FIXED PATH\n",
    "    target_size=(img_height, img_width), # FIXED SIZE (128x128)\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# 2. DEFINE MODEL (Updated for 128x128 input)\n",
    "model = Sequential([\n",
    "    # --- CNN PART ---\n",
    "    # Input Shape is now 128x128x3\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    # Optional: Add a 4th layer since 128x128 is deeper\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    # --- FCN PART ---\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output\n",
    "    Dense(train_ds.num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# 3. TRAIN\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"ğŸš€ Starting Training on HD Data...\")\n",
    "history = model.fit(train_ds, epochs=15, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3162bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined records: 50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Open and read the NDJSON file\n",
    "with open('D:\\\\MANAGER-ANGRY-METER\\\\video-input\\\\Audio.ndjson', 'r') as f:\n",
    "    data_audio = ndjson.load(f)\n",
    "\n",
    "\n",
    "print(f\"Combined records: {len(data_audio)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa3eb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cmhpke9p4cjbs0782sob057xx.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cmhpke9p4cjbt0782363n2x0u.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cmhpke9p4cjbu0782assgc5x9.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cmhpke9p4cjbv0782ah5bfrg4.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cmhpke9p4cjbw0782wkp7yx3s.mp4</td>\n",
       "      <td>[{'value': 'Surprised'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id                   emotion\n",
       "0  cmhpke9p4cjbs0782sob057xx.mp4  [{'value': 'Surprised'}]\n",
       "1  cmhpke9p4cjbt0782363n2x0u.mp4  [{'value': 'Surprised'}]\n",
       "2  cmhpke9p4cjbu0782assgc5x9.mp4  [{'value': 'Surprised'}]\n",
       "3  cmhpke9p4cjbv0782ah5bfrg4.mp4  [{'value': 'Surprised'}]\n",
       "4  cmhpke9p4cjbw0782wkp7yx3s.mp4  [{'value': 'Surprised'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta_audio = pd.DataFrame(data)\n",
    "df_meta_audio.drop(columns=['media_attributes'], inplace=True)\n",
    "df_meta_audio['data_row'] = df_meta_audio['data_row'].astype(str).str[8:37]\n",
    "df_meta_audio.rename(columns={'data_row': 'id'}, inplace=True)\n",
    "df_meta_audio.rename(columns={'metadata_fields': 'emotion'}, inplace=True)\n",
    "df_meta_audio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c161863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading Pretrained Wav2Vec2 Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5787e153f4fb45b583e2343c8d20d14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "Some layers from the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing TFWav2Vec2Model: ['lm_head', 'dropout_50']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFWav2Vec2Model were initialized from the model checkpoint at facebook/wav2vec2-base-960h.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Extracting Deep Features from 69 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 69/69 [00:00<00:00, 23030.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extraction Complete! Feature Shape: (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, TFWav2Vec2Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "video_folder = \"D:\\\\Manager-angry-meter\\\\data\\\\audio\"\n",
    "MODEL_NAME = \"facebook/wav2vec2-base-960h\" # The Industry Standard for Speech\n",
    "MAX_DURATION = 3 # seconds\n",
    "\n",
    "# 1. LOAD PRETRAINED MODEL (The \"Brain\")\n",
    "print(\"ğŸ“¥ Loading Pretrained Wav2Vec2 Model...\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_NAME)\n",
    "# We load the TensorFlow version of the model\n",
    "# output_hidden_states=True lets us peek inside the \"brain\"\n",
    "# Force it to use standard weights to avoid the iteration error\n",
    "base_model = TFWav2Vec2Model.from_pretrained(MODEL_NAME, use_safetensors=False)\n",
    "def extract_deep_features(path):\n",
    "    try:\n",
    "        # A. Load Audio (16kHz is mandatory for Wav2Vec2)\n",
    "        audio, sr = librosa.load(path, sr=16000, duration=MAX_DURATION)\n",
    "        \n",
    "        # B. Pad/Truncate to ensure consistent length\n",
    "        # 3 seconds * 16000 Hz = 48000 samples\n",
    "        target_len = 16000 * MAX_DURATION\n",
    "        if len(audio) < target_len:\n",
    "            audio = np.pad(audio, (0, target_len - len(audio)))\n",
    "        else:\n",
    "            audio = audio[:target_len]\n",
    "\n",
    "        # C. Prepare Input for the Model\n",
    "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"tf\")\n",
    "        \n",
    "        # D. Run through Pretrained Model\n",
    "        outputs = base_model(inputs.input_values)\n",
    "        \n",
    "        # E. Get the \"Embeddings\" (The last hidden state)\n",
    "        # Shape: (1, Sequence_Length, 768) -> We average it to get (768,)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        features = tf.reduce_mean(hidden_states, axis=1) \n",
    "        \n",
    "        return features.numpy().flatten()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# --- HARVEST LOOP ---\n",
    "deep_features = []\n",
    "labels = []\n",
    "\n",
    "print(f\"ğŸ§  Extracting Deep Features from {len(df_meta_audio)} videos...\")\n",
    "\n",
    "for index, row in tqdm(df_meta.iterrows(), total=len(df_meta)):\n",
    "    filename = row['id']\n",
    "    if not filename.endswith('.mp4'):\n",
    "        filename += \".mp4\"\n",
    "    path = os.path.join(video_folder, filename)\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        feats = extract_deep_features(path)\n",
    "        if feats is not None:\n",
    "            deep_features.append(feats)\n",
    "            labels.append(row['emotion'])\n",
    "\n",
    "# Save to disk so you don't have to run this again\n",
    "X_deep = np.array(deep_features)\n",
    "print(f\"âœ… Extraction Complete! Feature Shape: {X_deep.shape}\") \n",
    "# Shape should be (Num_Videos, 768) -> 768 is the size of Wav2Vec2 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Prepare Labels\n",
    "lb = LabelEncoder()\n",
    "y_deep = to_categorical(lb.fit_transform(labels))\n",
    "\n",
    "# 2. Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_deep, y_deep, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. BUILD YOUR CUSTOM MODEL\n",
    "# This is \"Adding the 2 layers\" on top of the pretrained knowledge\n",
    "model_transfer = Sequential([\n",
    "    # Input is 768 (The output of Wav2Vec2)\n",
    "    tf.keras.Input(shape=(768,)),\n",
    "    \n",
    "    # --- YOUR ADDED LAYERS ---\n",
    "    \n",
    "    # Added Layer 1\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4), # High dropout because Wav2Vec2 features are rich\n",
    "    \n",
    "    # Added Layer 2\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(y_deep.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# 4. Train\n",
    "model_transfer.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"ğŸš€ Training Transfer Learning Model...\")\n",
    "# This will be super fast because the \"Heavy Lifting\" (Wav2Vec2) is already done\n",
    "history = model_transfer.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))\n",
    "\n",
    "# 5. Save\n",
    "model_transfer.save('wav2vec2_transfer_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6476da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- DEFINE MODEL ---\n",
    "model_audio = Sequential()\n",
    "\n",
    "# 1. Input Layer (Matches your extracted features)\n",
    "model_audio.add(tf.keras.Input(shape=(X.shape[1],)))\n",
    "\n",
    "# 2. First FCN Layer (Hidden Layer 1)\n",
    "model_audio.add(Dense(256, activation='relu'))\n",
    "model_audio.add(BatchNormalization()) # Helps training stability\n",
    "model_audio.add(Dropout(0.3))         # Prevents overfitting\n",
    "\n",
    "# 3. Second FCN Layer (Hidden Layer 2 - as requested)\n",
    "model_audio.add(Dense(128, activation='relu'))\n",
    "model_audio.add(BatchNormalization())\n",
    "model_audio.add(Dropout(0.3))\n",
    "\n",
    "# 4. Output Layer\n",
    "model_audio.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# --- COMPILE & TRAIN ---\n",
    "model_audio.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"ğŸš€ Starting Audio FCN Training...\")\n",
    "history_audio = model_audio.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
